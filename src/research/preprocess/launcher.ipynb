{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me if working on local machine\n",
    "import os\n",
    "import sys\n",
    "os.chdir(\"../../..\")\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tolya/Documents/code/dlrm'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"notebooks.debug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.logging\n",
    "utils.logging.setup(\"conf/logging/default.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-26 01:13:35,421 - utils.configs - INFO - loading app config 'conf/app.yaml'...\n",
      "2023-11-26 01:13:35,423 - utils.configs - INFO - loading app config 'conf/app.yaml': done\n"
     ]
    }
   ],
   "source": [
    "import utils.configs\n",
    "_ = utils.configs.setup(\"conf/app.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "assert dotenv.load_dotenv(dotenv_path=\"conf/envs/dev.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow\n",
    "import pyarrow.dataset\n",
    "import utils.aws.s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = utils.configs.get(\"aws.s3.bucket\")\n",
    "s3_prefix = utils.configs.get(\"aws.s3.prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_dir = os.path.join(\"s3://\", s3_bucket, s3_prefix, \"joined\", \"compact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-26 01:13:36,168 - botocore.credentials - INFO - Found credentials in environment variables.\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/tolya/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-26 01:13:37,285 - botocore.credentials - INFO - Found credentials in environment variables.\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/tolya/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "boto_session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_processor(\n",
    "    instance_count = 1,\n",
    "    instance_type = \"ml.m5.xlarge\",\n",
    "    volume_size_in_gb = 30,\n",
    "    max_runtime_in_seconds = 2*60*60,\n",
    "    framework_version = \"2.4\",\n",
    "    spark_app_name = \"dlrm-etl\",\n",
    "    sagemaker_role = utils.configs.get(\"aws.sagemaker.role\"),\n",
    "    tags = utils.configs.get(\"aws.sagemaker.tags\")\n",
    "):\n",
    "    spark_processor = PySparkProcessor(\n",
    "        sagemaker_session = sagemaker_session,\n",
    "        base_job_name = spark_app_name,\n",
    "        role = sagemaker_role,\n",
    "        instance_count = instance_count,\n",
    "        instance_type = instance_type,\n",
    "        framework_version = framework_version,\n",
    "        max_runtime_in_seconds = max_runtime_in_seconds,\n",
    "        volume_size_in_gb = volume_size_in_gb,\n",
    "        tags = tags,\n",
    "    )\n",
    "    return spark_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in range(12, 20):\n",
    "    day = str(day).zfill(2)\n",
    "    spark_processor = create_spark_processor(instance_count=1)\n",
    "    spark_processor.run(\n",
    "        submit_app=\"/app/code/research/cloud/sagemaker/01-repack.py\",\n",
    "        arguments=[\n",
    "            \"--src-path\", os.path.join(\"s3://\", s3_bucket, s3_prefix, \"gz\", f\"day={day}\"),\n",
    "            \"--dst-path\", os.path.join(\"s3://\", s3_bucket, s3_prefix, \"pq\", f\"day={day}\"),\n",
    "        ],\n",
    "        spark_event_logs_s3_uri = utils.configs.get(\"aws.sagemaker.spark_logs_uri\"),\n",
    "        wait = False,\n",
    "        logs = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_processor = create_spark_processor(instance_count=8, volume_size_in_gb=300, max_runtime_in_seconds=12*60*60)\n",
    "spark_processor.run(\n",
    "    submit_app=\"/app/code/research/cloud/sagemaker/02-parse.py\",\n",
    "    arguments=[\n",
    "        \"--src-path\", os.path.join(\"s3://\", s3_bucket, s3_prefix, \"pq\"),\n",
    "        \"--dst-path\", os.path.join(\"s3://\", s3_bucket, s3_prefix, \"parsed\"),\n",
    "        # \"--day\", \"0\"\n",
    "    ],\n",
    "    spark_event_logs_s3_uri = utils.configs.get(\"aws.sagemaker.spark_logs_uri\"),\n",
    "    wait = True,\n",
    "    logs = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_processor = create_spark_processor(instance_type=\"ml.m5.4xlarge\", instance_count=1, volume_size_in_gb=20, max_runtime_in_seconds=2*60*60)\n",
    "\n",
    "for feature_idx in range(14, 40):\n",
    "    logger.info(f\"spawning dict job for feature {feature_idx}...\")\n",
    "    spark_processor.run(\n",
    "        submit_app=\"/app/code/research/cloud/sagemaker/03-build-dicts.py\",\n",
    "        arguments=[\n",
    "            \"--src-path\", os.path.join(\"s3://\", s3_bucket, s3_prefix, \"transformed\"),\n",
    "            \"--dst-path\", os.path.join(\"s3://\", s3_bucket, s3_prefix, \"dicts\"),\n",
    "            \"--feature-name\", f\"f{feature_idx}\"\n",
    "        ],\n",
    "        spark_event_logs_s3_uri = utils.configs.get(\"aws.sagemaker.spark_logs_uri\"),\n",
    "        wait = False,\n",
    "        logs = False\n",
    "    )\n",
    "    logger.info(f\"spawning dict job for feature {feature_idx}: done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_processor = create_spark_processor(instance_type=\"ml.m5.4xlarge\", instance_count=8, volume_size_in_gb=300, max_runtime_in_seconds=5*60*60)\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"/app/code/research/cloud/sagemaker/04-join-compact.py\",\n",
    "    arguments=[\n",
    "        \"--src-path-logs\", os.path.join(\"s3://\", s3_bucket, s3_prefix, \"transformed\"),\n",
    "        \"--src-path-dicts\", os.path.join(\"s3://\", s3_bucket, s3_prefix, \"dicts\"),\n",
    "        \"--dst-path\", os.path.join(\"s3://\", s3_bucket, s3_prefix, \"joined/compact\"),\n",
    "        # \"--date\", \"2023-01-01\",\n",
    "        \"--repartition\", \"10\",\n",
    "        \"--freq-threshold-abs\", \"100000\",\n",
    "        \"--freq-threshold-pct\", \"0.95\",\n",
    "    ],\n",
    "    spark_event_logs_s3_uri = utils.configs.get(\"aws.sagemaker.spark_logs_uri\"),\n",
    "    wait = False,\n",
    "    logs = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure data read speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm.auto as tqdm\n",
    "import pyarrow\n",
    "import pyarrow.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_speed(\n",
    "    dataset,\n",
    "    filter,\n",
    "    batch_size = 10_000,\n",
    "    limit = None\n",
    "):\n",
    "    if limit is None:\n",
    "        logger.info(\"getting dataset size...\")\n",
    "        total_records = dataset.count_rows(filter=filter)\n",
    "        logger.info(f\"getting dataset size: done ({total_records} records)\")\n",
    "    else:\n",
    "        total_records = limit\n",
    "\n",
    "    logger.info(\"reading dataset...\")\n",
    "    time_start = time.time()\n",
    "    pbar = tqdm.tqdm(desc=\"reading data\", total=total_records)\n",
    "    src_batches = dataset.to_batches(filter=filter, batch_size=batch_size)\n",
    "    rows_processed = 0\n",
    "    for batch_id, batch in enumerate(src_batches, start=1):\n",
    "        batch = batch.to_pandas()\n",
    "        pbar.set_postfix({'batches': batch_id}, refresh=False)\n",
    "        pbar.update(batch.shape[0])\n",
    "        rows_processed += batch.shape[0]\n",
    "        if limit is not None and rows_processed >= limit:\n",
    "            break\n",
    "    pbar.close()\n",
    "\n",
    "    time_finish = time.time()\n",
    "    elapsed_time = (time_finish - time_start)\n",
    "    read_speed = rows_processed / elapsed_time\n",
    "    logger.info(f\"reading dataset: done ({int(elapsed_time)} seconds, {int(read_speed)} rows/sec)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195841983"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dataset = pyarrow.dataset.dataset(\n",
    "    s3_dir,\n",
    "    partitioning = \"hive\"\n",
    ")\n",
    "src_filter = (pyarrow.dataset.field(\"date\") == \"2023-01-01\")\n",
    "src_rows = src_dataset.count_rows(filter=src_filter)\n",
    "src_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(src_dataset.to_batches(filter=src_filter))).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_speed(src_dataset, src_filter, limit=src_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
