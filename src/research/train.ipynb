{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"notebooks.debug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.logging\n",
    "utils.logging.setup(\"conf/logging/default.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.configs\n",
    "_ = utils.configs.setup(\"conf/app.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "assert dotenv.load_dotenv(dotenv_path=\"conf/envs/dev.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import tqdm.auto as tqdm\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import pyarrow.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = \"label\"\n",
    "DENSE_FEATURES = [f\"f{idx}\" for idx in range(1,14)]\n",
    "SPARSE_FEATURES = [f\"f{idx}_idx\" for idx in range(14,40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dataset = pyarrow.dataset.dataset(\n",
    "    \"data/joined/compact\",\n",
    "    partitioning = \"hive\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_speed(\n",
    "    dataset,\n",
    "    filter = None,\n",
    "    limit = None,\n",
    "    read_params = {},\n",
    "    payload_fn = None\n",
    "):\n",
    "    if limit is None:\n",
    "        logger.info(\"getting dataset size...\")\n",
    "        total_records = dataset.count_rows(filter=filter)\n",
    "        logger.info(f\"getting dataset size: done ({total_records} records)\")\n",
    "    else:\n",
    "        total_records = limit\n",
    "\n",
    "    logger.info(\"reading dataset...\")\n",
    "    time_start = time.time()\n",
    "    pbar = tqdm.tqdm(desc=\"reading data\", total=total_records)\n",
    "    src_batches = dataset.to_batches(filter=filter, **read_params)\n",
    "    rows_processed = 0\n",
    "    for batch_id, batch in enumerate(src_batches, start=1):\n",
    "        batch = batch.to_pandas()\n",
    "        pbar.set_postfix({'batches': batch_id}, refresh=False)\n",
    "        pbar.update(batch.shape[0])\n",
    "        rows_processed += batch.shape[0]\n",
    "        if limit is not None and rows_processed >= limit:\n",
    "            break\n",
    "        if payload_fn is not None:\n",
    "            payload_fn(batch)\n",
    "    pbar.close()\n",
    "\n",
    "    time_finish = time.time()\n",
    "    elapsed_time = (time_finish - time_start)\n",
    "    read_speed = rows_processed / elapsed_time\n",
    "    logger.info(f\"reading dataset: done ({int(elapsed_time)} seconds, {int(read_speed)} rows/sec)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_features(batch, device = \"cpu\"):\n",
    "    dense_features = torch.log(torch.tensor(batch[DENSE_FEATURES].to_numpy(dtype=\"float32\"), device=device) + 3)\n",
    "    dense_features.masked_fill_(dense_features.isnan(), 0)\n",
    "    return dense_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_features(batch, device = \"cpu\"):\n",
    "    sparse_features = torch.tensor(batch[SPARSE_FEATURES].to_numpy(dtype=\"int32\"), device=device)\n",
    "    return sparse_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(batch, device = \"cpu\"):\n",
    "    labels = torch.tensor(batch[LABEL].to_numpy(dtype=\"int8\"), device=device)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_batch(batch, device = \"cpu\"):\n",
    "    dense_features = get_dense_features(batch, device)\n",
    "    sparse_features = get_sparse_features(batch, device)\n",
    "    labels = get_labels(batch, device)\n",
    "    return dense_features, sparse_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure_speed(src_dataset) ## warmup the disk cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure_speed(src_dataset, payload_fn = lambda b: convert_batch(b, device=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure_speed(src_dataset, payload_fn = lambda b: convert_batch(b, device=\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_cardinality(dataset):\n",
    "    max_ids = []\n",
    "    pbar = tqdm.tqdm(\"reading dataset\", total=dataset.count_rows())\n",
    "    for batch in src_dataset.to_batches():\n",
    "        batch = batch.to_pandas()\n",
    "        pbar.update(batch.shape[0])\n",
    "        max_ids.append(get_sparse_features(batch, device = \"mps\").max(dim=0).values)\n",
    "    return torch.stack(max_ids).max(dim=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feature_sizes = get_feature_cardinality(src_dataset)\n",
    "sparse_feature_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feature_sizes = dict(zip(SPARSE_FEATURES, list(sparse_feature_sizes.data.cpu().numpy())))\n",
    "sparse_feature_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing aroudn w/ torch components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "DENSE_LAYERS = [512,256,EMBEDDING_DIM]\n",
    "FINAL_LAYERS = [512,512,256,1]\n",
    "DEVICE = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_id():\n",
    "    return f\"exp-{datetime.datetime.now().replace(microsecond=0).isoformat()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_INTERVAL = 1\n",
    "VAL_INTERVAL = LOG_INTERVAL * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.dlrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.dlrm.DLRM(\n",
    "    sparse_feature_dim = EMBEDDING_DIM,\n",
    "    sparse_feature_sizes = [size+100 for size in list(sparse_feature_sizes.values())],\n",
    "    dense_in_features = len(DENSE_FEATURES),\n",
    "    dense_layer_sizes = DENSE_LAYERS,\n",
    "    final_layer_sizes = FINAL_LAYERS,\n",
    "    dense_device = DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tensorboard.SummaryWriter(log_dir=f\"data/exps/{exp_id()}\")\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "pbar = tqdm.tqdm(\"training\", total=src_dataset.count_rows())\n",
    "for batch_id, batch in enumerate(src_dataset.to_batches(), start=1):\n",
    "    batch = batch.to_pandas()\n",
    "    dense_features, sparse_features, labels = convert_batch(batch)\n",
    "    logits = model(dense_features, sparse_features)\n",
    "    loss = loss_fn(logits.squeeze(-1), labels.to(DEVICE).float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    pbar.set_postfix({'batches': batch_id, 'loss': loss.item()}, refresh=False)\n",
    "    if batch_id % LOG_INTERVAL == 0:\n",
    "        writer.add_scalar('loss/train', loss.item(), batch_id)\n",
    "    pbar.update(batch.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
