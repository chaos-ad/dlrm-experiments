{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tolya/Documents/code/dlrm/src/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tolya/Documents/code/dlrm'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"notebooks.debug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.logging\n",
    "utils.logging.setup(\"conf/logging/default.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-26 00:28:53,529 - utils.configs - INFO - loading app config 'conf/app.yaml'...\n",
      "2023-11-26 00:28:53,531 - utils.configs - INFO - loading app config 'conf/app.yaml': done\n"
     ]
    }
   ],
   "source": [
    "import utils.configs\n",
    "_ = utils.configs.setup(\"conf/app.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "assert dotenv.load_dotenv(dotenv_path=\"conf/envs/dev.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tolya/Documents/code/dlrm/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "import tqdm.auto as tqdm\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import pyarrow.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = \"label\"\n",
    "DENSE_FEATURES = [f\"f{idx}\" for idx in range(1,14)]\n",
    "SPARSE_FEATURES = [f\"f{idx}_idx\" for idx in range(14,40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dataset = pyarrow.dataset.dataset(\n",
    "    \"data/joined/compact\",\n",
    "    partitioning = \"hive\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_speed(\n",
    "    dataset,\n",
    "    filter = None,\n",
    "    limit = None,\n",
    "    read_params = {},\n",
    "    payload_fn = None\n",
    "):\n",
    "    if limit is None:\n",
    "        logger.info(\"getting dataset size...\")\n",
    "        total_records = dataset.count_rows(filter=filter)\n",
    "        logger.info(f\"getting dataset size: done ({total_records} records)\")\n",
    "    else:\n",
    "        total_records = limit\n",
    "\n",
    "    logger.info(\"reading dataset...\")\n",
    "    time_start = time.time()\n",
    "    pbar = tqdm.tqdm(desc=\"reading data\", total=total_records)\n",
    "    src_batches = dataset.to_batches(filter=filter, **read_params)\n",
    "    rows_processed = 0\n",
    "    for batch_id, batch in enumerate(src_batches, start=1):\n",
    "        batch = batch.to_pandas()\n",
    "        pbar.set_postfix({'batches': batch_id}, refresh=False)\n",
    "        pbar.update(batch.shape[0])\n",
    "        rows_processed += batch.shape[0]\n",
    "        if limit is not None and rows_processed >= limit:\n",
    "            break\n",
    "        if payload_fn is not None:\n",
    "            payload_fn(batch)\n",
    "    pbar.close()\n",
    "\n",
    "    time_finish = time.time()\n",
    "    elapsed_time = (time_finish - time_start)\n",
    "    read_speed = rows_processed / elapsed_time\n",
    "    logger.info(f\"reading dataset: done ({int(elapsed_time)} seconds, {int(read_speed)} rows/sec)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_features(batch, device = \"cpu\"):\n",
    "    dense_features = torch.log(torch.tensor(batch[DENSE_FEATURES].to_numpy(dtype=\"float32\"), device=device) + 3)\n",
    "    dense_features.masked_fill_(dense_features.isnan(), 0)\n",
    "    return dense_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_features(batch, device = \"cpu\"):\n",
    "    sparse_features = torch.tensor(batch[SPARSE_FEATURES].to_numpy(dtype=\"int32\"), device=device)\n",
    "    return sparse_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(batch, device = \"cpu\"):\n",
    "    labels = torch.tensor(batch[LABEL].to_numpy(dtype=\"int8\"), device=device)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_batch(batch, device = \"cpu\"):\n",
    "    dense_features = get_dense_features(batch, device)\n",
    "    sparse_features = get_sparse_features(batch, device)\n",
    "    labels = get_labels(batch, device)\n",
    "    return dense_features, sparse_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure_speed(src_dataset) ## warmup the disk cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure_speed(src_dataset, payload_fn = lambda b: convert_batch(b, device=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure_speed(src_dataset, payload_fn = lambda b: convert_batch(b, device=\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_cardinality(dataset):\n",
    "    max_ids = []\n",
    "    pbar = tqdm.tqdm(\"reading dataset\", total=dataset.count_rows())\n",
    "    for batch in src_dataset.to_batches():\n",
    "        batch = batch.to_pandas()\n",
    "        pbar.update(batch.shape[0])\n",
    "        max_ids.append(get_sparse_features(batch, device = \"mps\").max(dim=0).values)\n",
    "    return torch.stack(max_ids).max(dim=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195841983/195841983 [00:19<00:00, 10170011.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([100000,   4460,   7123,    393,   5313,      2,   3353,    356,      8,\n",
       "        100001,  16640,  19153,      8,    261,   1515,     11,      3,     20,\n",
       "             7, 100000, 100001, 100001,  18156,   4112,      8,     10],\n",
       "       device='mps:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_feature_sizes = get_feature_cardinality(src_dataset)\n",
    "sparse_feature_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f14_idx': 100000,\n",
       " 'f15_idx': 4460,\n",
       " 'f16_idx': 7123,\n",
       " 'f17_idx': 393,\n",
       " 'f18_idx': 5313,\n",
       " 'f19_idx': 2,\n",
       " 'f20_idx': 3353,\n",
       " 'f21_idx': 356,\n",
       " 'f22_idx': 8,\n",
       " 'f23_idx': 100001,\n",
       " 'f24_idx': 16640,\n",
       " 'f25_idx': 19153,\n",
       " 'f26_idx': 8,\n",
       " 'f27_idx': 261,\n",
       " 'f28_idx': 1515,\n",
       " 'f29_idx': 11,\n",
       " 'f30_idx': 3,\n",
       " 'f31_idx': 20,\n",
       " 'f32_idx': 7,\n",
       " 'f33_idx': 100000,\n",
       " 'f34_idx': 100001,\n",
       " 'f35_idx': 100001,\n",
       " 'f36_idx': 18156,\n",
       " 'f37_idx': 4112,\n",
       " 'f38_idx': 8,\n",
       " 'f39_idx': 10}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_feature_sizes = dict(zip(SPARSE_FEATURES, list(sparse_feature_sizes.data.cpu().numpy())))\n",
    "sparse_feature_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing aroudn w/ torch components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "DENSE_LAYERS = [512,256,EMBEDDING_DIM]\n",
    "FINAL_LAYERS = [512,512,256,1]\n",
    "DEVICE = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_id():\n",
    "    return f\"exp-{datetime.datetime.now().replace(microsecond=0).isoformat()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exp-2023-11-26T00:29:13'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_INTERVAL = 1\n",
    "VAL_INTERVAL = LOG_INTERVAL * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.dlrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.dlrm.DLRM(\n",
    "    sparse_feature_dim = EMBEDDING_DIM,\n",
    "    sparse_feature_sizes = [size+100 for size in list(sparse_feature_sizes.values())],\n",
    "    dense_in_features = len(DENSE_FEATURES),\n",
    "    dense_layer_sizes = DENSE_LAYERS,\n",
    "    final_layer_sizes = FINAL_LAYERS,\n",
    "    dense_device = DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195841983/195841983 [3:33:44<00:00, 27397.09it/s, batches=1510, loss=0.127]  "
     ]
    }
   ],
   "source": [
    "writer = tensorboard.SummaryWriter(log_dir=f\"data/exps/{exp_id()}\")\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "pbar = tqdm.tqdm(\"training\", total=src_dataset.count_rows())\n",
    "for batch_id, batch in enumerate(src_dataset.to_batches(), start=1):\n",
    "    batch = batch.to_pandas()\n",
    "    dense_features, sparse_features, labels = convert_batch(batch)\n",
    "    logits = model(dense_features, sparse_features)\n",
    "    loss = loss_fn(logits.squeeze(-1), labels.to(DEVICE).float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    pbar.set_postfix({'batches': batch_id, 'loss': loss.item()}, refresh=False)\n",
    "    if batch_id % LOG_INTERVAL == 0:\n",
    "        writer.add_scalar('loss/train', loss.item(), batch_id)\n",
    "    pbar.update(batch.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-26 13:02:40,961 - utils.filesystem - INFO - creating path 'data/checkpoints/exp-2023-11-26T00:29:14'...\n",
      "2023-11-26 13:02:40,965 - utils.filesystem - INFO - creating path 'data/checkpoints/exp-2023-11-26T00:29:14': done\n"
     ]
    }
   ],
   "source": [
    "import utils.filesystem\n",
    "utils.filesystem.mkdir(\"data/checkpoints/exp-2023-11-26T00:29:14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {}\n",
    "checkpoint['model_state_dict'] = model.state_dict()\n",
    "checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
    "checkpoint_path = \"data/checkpoints/exp-2023-11-26T00:29:14/first-attempt.pt\"\n",
    "torch.save(checkpoint, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "model = models.dlrm.DLRM(\n",
    "    sparse_feature_dim = EMBEDDING_DIM,\n",
    "    sparse_feature_sizes = [size+100 for size in list(sparse_feature_sizes.values())],\n",
    "    dense_in_features = len(DENSE_FEATURES),\n",
    "    dense_layer_sizes = DENSE_LAYERS,\n",
    "    final_layer_sizes = FINAL_LAYERS,\n",
    "    dense_device = DEVICE\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3739,  0.0116, -0.7886, -0.1465,  0.9212, -0.4239, -0.2930,  0.1731,\n",
       "        -0.1319, -1.0849,  0.1730, -2.4127, -0.6039,  0.7543,  0.8656,  1.3840,\n",
       "         0.5706, -0.1873, -0.1842,  0.4883,  0.7234, -1.1200,  0.7445,  0.0559,\n",
       "         1.2161,  1.0311, -0.3917,  0.2094,  0.9472, -0.1171,  0.5646, -0.9953,\n",
       "        -0.2787,  0.3100,  0.5355, -0.4890,  0.3734,  0.6972,  1.1223,  0.1922,\n",
       "        -0.8874,  0.9448, -0.4160,  0.6944,  1.1935, -0.1178,  0.4508,  0.2079,\n",
       "        -0.5156, -1.0846, -0.5752, -1.1570, -0.9603, -0.2282, -1.0797,  0.3148,\n",
       "         1.2818,  1.3543,  0.4588, -1.2980,  1.2508, -1.2206,  0.1699,  0.0982],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sparse_arch.embeddings[0].weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
